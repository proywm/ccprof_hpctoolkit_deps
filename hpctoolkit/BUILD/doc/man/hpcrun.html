<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!-- Manual page created with latex2man on Mon Nov 20 15:36:21 EST 2017 --
-- Author of latex2man: Juergen.Vollmer@informatik-vollmer.de --
-- NOTE: This file is generated, DO NOT EDIT. -->
<html>
<head><title>HPCRUN</title></head>
<body bgcolor="white">
<h1 align=center>
hpcrun:<br>
Statistical Profiling 
</h1>
<h4 align=center>The HPCToolkit Performance Tools </h4>
<h4 align=center> 2014/03/05</h4>
<h4 align=center>Version 2017.11</h4>
<p>
<tt>hpcrun</tt>
is a call path profiler based on statistical sampling. 
It supports multiple sample sources during one execution. 
<tt>hpcrun</tt>
profiles complex applications (forks, execs, threads and dynamic linking) and may be used in conjunction with parallel process launchers such as MPICH's <tt>mpiexec</tt>
and SLURM's <tt>srun</tt>.
<p>
See <a href="hpctoolkit.html"><em>hpctoolkit</em>(1)
</a>for an overview of <strong>HPCToolkit</strong>.
<p>
<h3>Table of Contents</h3>
<ul>
<li><a href="#section_1">Synopsis </a></li>
<li><a href="#section_2">Description </a></li>
<li><a href="#section_3">Arguments </a>
<ul>
<li><a href="#section_4">Options: Informational </a></li>
<li><a href="#section_5">Options: Profiling </a></li>
</ul>
<li><a href="#section_6">Environment Variables </a></li>
<li><a href="#section_7">Examples </a></li>
<li><a href="#section_8">Notes </a>
<ul>
<li><a href="#section_9">Sample sources </a></li>
<li><a href="#section_10">Platform-specific notes </a>
<ul>
<li><a href="#section_11">Cray XE and XK </a></li>
</ul>
<li><a href="#section_12">Miscellaneous </a></li>
</ul>
<li><a href="#section_13">See Also </a></li>
<li><a href="#section_14">Version </a></li>
<li><a href="#section_15">License and Copyright </a></li>
<li><a href="#section_16">Authors </a></li>
</ul>
<p>
<h2><a name="section_1">Synopsis</a></h2>

<p>
<tt>hpcrun</tt>
[<b>profiling-options</b>]
<i>command</i>
[<b>command-arguments</b>]
<p>
<tt>hpcrun</tt>
[<b>info-options</b>]
<p>
<h2><a name="section_2">Description</a></h2>

<p>
<tt>hpcrun</tt>
profiles the execution of an arbitrary command <i>command</i>
using statistical sampling (rather than instrumentation). 
It collects per-thread call path profiles that represent the full calling context of sample points. 
Sample points may be generated from multiple simultaneous sampling sources. 
<tt>hpcrun</tt>
profiles complex applications that use forks, execs, threads, and dynamic linking/unlinking; it may be used in conjuction with parallel process launchers such as MPICH's <tt>mpiexec</tt>
and SLURM's <tt>srun</tt>.
<p>
To profile a statically linked executable, make sure to link with <a href="hpclink.html"><em>hpclink</em>(1)
</a>.
<p>
To configure <tt>hpcrun</tt>'s
sampling sources, specify events and periods using the <tt>-e/--event</tt>
option. 
For an event <em>e</em>
and period <em>p</em>,
after every <em>p</em>
instances of <em>e</em>,
a sample is generated that causes <tt>hpcrun</tt>
to inspect the and record information about the monitored <i>command</i>.
<p>
When <i>command</i>
terminates, a profile measurement databse will be written to the directory:<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hpctoolkit-<i>command</i>-measurements[-<i>jobid</i>]<br>

where <i>jobid</i>
is a PBS or Sun Grid Engine job identifier. 
<p>
<tt>hpcrun</tt>
enables a user to abort a process and write the partial profiling data to disk by sending the Interrupt signal (INT or Ctrl-C). 
This can be extremely useful on long-running or misbehaving applications. 
<p>
<h2><a name="section_3">Arguments</a></h2>

<p>
<dl compact>
<dt><i>command</i>
</dt>
<dd> The command to profile. 
</dd>
<dt><i>command-arguments</i>
</dt>
<dd> Arguments to the command to profile. 
</dd>
</dl>
<p>
Default values for an option's optional arguments are shown in {}. 
<p>
<h4><a name="section_4">Options: Informational</a></h4>

<p>
<dl compact>
<dt><b>-l</b>, <b>-L</b>, <b>--list-events</b>
</dt>
<dd> 
List available events. (N.B.: some may not be profilable) 
<p>
</dd>
<dt><b>-V</b>, <b>--version</b>
</dt>
<dd> 
Print version information. 
<p>
</dd>
<dt><b>-h</b>, <b>--help</b>
</dt>
<dd> 
Print help. 
</dd>
</dl>
<p>
<h4><a name="section_5">Options: Profiling</a></h4>

<p>
<dl compact>
<p>
<dt><b>-e</b> <i>event[@period]</i>, <b>--event</b> <i>event[@period]</i>
</dt>
<dd> 
An event to profile and its corresponding sample period. <i>event</i>
may be either a PAPI, native processor event, or WALLCLOCK (microseconds). May pass multiple times as implementations permits. {<tt>WALLCLOCK@5000</tt>} 
<ul compact>
<li>It is recommended to always specify the sampling period for each profiling event. 
</li>
<li>The special event WALLCLOCK may be used to profile the `wall clock.' It may be used only <em>once</em> and cannot be used with another event. Its period is in microseconds.
</li>
<li>Hardware and drivers 1) limit the maximum number of events that can be monitored simultaneously, and 2) forbid certain combinations of events. 
</li>
<li>See the ``Sample sources'' under <strong>NOTES</strong> for additional details.
</li>
</ul>
<p>
</dd>
<dt><b>-t</b>, <b>--trace</b>
</dt>
<dd> 
Generate a call path trace (in addition to a call path profile). 
<p>
</dd>
<dt><b>-f</b> <i>frac</i>, <b>-fp</b> <i>frac</i>, <b>--process-fraction</b> <i>frac</i>
</dt>
<dd> 
Measure only a fraction <i>frac</i>
of the execution's processes. 
For each process, enable measurement (of all threads) with probability <i>frac</i>;
<i>frac</i>
is a real number (0.10) or a fraction (1/10) between 0 and 1. 
(To minimize perturbations, when a process is disabled, all threads in a process still receive sampling interrupts, but they are ignored.) 
<p>
</dd>
<dt><b>-r</b>, <b>--retain-recursion</b>
</dt>
<dd> 
Normally, hpcrun will collapse (simple) recursive call chains 
to a single node. This option disables that behavior: all 
elements of a recursive call chain are recorded 
NOTE: If the user employs the RETCNT sample source, then this 
option is enabled: RETCNT implies *all* elements of 
call chains, including recursive elements, are recorded. 
<p>
</dd>
<dt><b>-o</b> <i>outpath</i>, <b>--output</b> <i>outpath</i>
</dt>
<dd> 
Directory for output data. 
{<tt>hpctoolkit-&lt;command&gt;-measurements[-&lt;jobid&gt;]</tt>} 
<p>
<ul compact>
Bug: Without a <i>jobid</i>
or an output option, multiple profiles of the same <command> will be placed in the same output directory. 
</li>
</ul>
<p>
</dd>
</dl>
<p>
<h2><a name="section_6">Environment Variables</a></h2>

For most systems, <tt>hpcrun</tt>
requires no special environment variable settings. 
There are two situations, however, where <tt>hpcrun</tt>,
to function correctly, 
<em>must</em>
refer to environment variables. These environment variables, and 
corresponding situations are: 
<dl compact>
[<tt>HPCTOOLKIT</tt>] To function correctly, <tt>hpcrun</tt>
must know 
the location of the <tt>HPCToolkit</tt>
top-level installation directory. 
The <tt>hpcrun</tt>
script uses elements of the installation <tt>lib</tt>
and 
<tt>libexec</tt>
subdirectories. For most systems, the 
installation procedure ensures that <tt>hpcrun</tt>
can find the requisite 
components. Some parallel job launchers, however, will <em>copy</em>
the 
<tt>hpcrun</tt>
script to a different location from the installed base. If your 
system uses this copying mechanism, you must set the <tt>HPCTOOLKIT</tt> 
environment variable to the top-level installation directory. 
</dd>
</dl>
<p>
<h2><a name="section_7">Examples</a></h2>

<p>
Assume we wish to profile the application <tt>zoo</tt>.
The following examples lists some useful events for different processor architectures. 
In each case, the special option <tt>--</tt>
is used to clearly demarcate the end of <tt>hpcrun</tt>
options. 
<p>
<ol compact>
<li value =1><tt>hpcrun -e WALLCLOCK@5000 zoo</tt> 
<p>
</li>
<li value =2>Opteron, (Rev B-F) 
<ol compact>
<li value =1><tt>hpcrun -e DC_L2_REFILL@1300013 -e PAPI_L2_DCM@510011 -e PAPI_STL_ICY@5300013 -e PAPI_TOT_CYC@13000019 zoo</tt> (<tt>DC_L2_REFILL</tt> is an approximation of L1 D-cache misses). 
</li>
<li value =2><tt>hpcrun -e PAPI_L2_DCM@510011 -e PAPI_TLB_DM@510013 -e PAPI_STL_ICY@5300013 -e PAPI_TOT_CYC@13000019 zoo</tt> 
</li>
</ol>
<p>
</li>
</ol>
<p>
<h2><a name="section_8">Notes</a></h2>

<p>
<h4><a name="section_9">Sample sources</a></h4>

<p>
<tt>hpcrun</tt>
uses the PAPI library to provide access to hardware performance counter events. 
If you have not configured HPCToolkit to use the PAPI library, you will be unable to measure hardware performance counter events. 
<p>
The PAPI library supports a large collection of hardware counter events. 
Some events have standard names across all platforms, e.g. <tt>PAPI_TOT_CYC</tt>, the event that measures total cycles. 
In addition to events whose names begin with the <tt>PAPI_</tt> prefix, platforms also provide access to a set of native events with names that are specific to the platform's processor. 
A complete list of events supported by the PAPI library for your platform may be obtained by using the <tt>--list-events</tt>
option. 
Any event whose name begins with the <tt>PAPI_</tt> prefix that is listed as "Profilable" can be used as an event in a sampling source --- provided it does not conflict with another event. 
<p>
The precise rules for selecting good events and periods are complex. 
<ul compact>
<p>
<li><strong>Choosing sampling events.</strong>
We recommend using PAPI events in general. 
However, some PAPI events are not profilable because of PAPI implementation details. 
Also, PAPI's standard event list may not cover an architectural feature you are interested in. 
In such cases, it is necessary to resort to native events. 
In many cases, you will have to consult the architecture's manual to fully understand what the event means: there is no standard event list or naming scheme and events sometimes have unusual meanings. 
<p>
</li>
<li><strong>Number of sampling events.</strong>
Currently, hpcrun does not multiplex hardware counters. 
This means that the number of events that you may concurrently profile against is limited by your architecture's performance monitoring unit. 
Note that some architectures hard-wire one or more counters to a specific event (such as cycles). 
<p>
</li>
<li><strong>Choosing sampling periods.</strong>
The key requirement in choosing sampling periods is that you obtain enough samples to provide statistical significance. 
We usually recommend a sampling rate between 100s-1000s of samples per second. 
This usually only produces 1-5% execution time overhead. 
<p>
Choosing sampling rates depends on the architecture and sometimes the application. 
<p>
Choosing periods for cycle and instruction-related events are usually easy. 
Cycles directly relates to the clock speed. 
Instruction-related events relates to the issue rate and width. 
<p>
Choosing periods for other events seems harder because different applications uses resources differently. 
For example, some applications are memory intensive and others are not. 
However, if the goal is to identify rate-limiting factors of the architecture, then it is not necessary to consider the application. 
For example, if the goal is to determine whether L2 D-cache latency is a limiting factor, then it is only necessary to work backward from the architecture's specifications to determine what number of L2 D-cache misses per second would be problematic. 
<p>
</li>
<li><strong>Architectural event conflicts.</strong>
With some performance monitoring units, certain events may not be concurrently used with other events. 
<p>
</li>
<li><strong>Architectural interrupt delay.</strong>
On most out-of-order pipelined architectures, a hardware counter interrupt is not precisely attributed to the instruction that induced the counter to overflow. 
The gap is commonly 50-70 instructions. 
This means that one should not assume that aggregation at the source line level is fully precise. 
(E.g., if a L1 D-cache miss is attributed to a statement that has been compiled to register-only operations, assume the miss is attributed to a nearby load.) 
However, aggregation at the procedure and loop level is reliable. 
<p>
</li>
<li><strong>System itimer (WALLCLOCK).</strong>
On Linux systems, the kernel will not deliver itimer interrupts faster than the unit of a jiffy, which defaults to 4 milliseconds; see the itimer man page. 
One can configure the kernel to use a value as small as 1 millisecond, but it is unlikely the kernel will actually deliver itimer signals at that rate when a period of 1000 microseconds is requested. 
<p>
However, on Linux one can get quite close to the kernel Hz rate by setting the itimer interval to something less than the Hz rate. 
For example, if the Hz rate is 1000 microseconds, one can use 500 microseconds (or just 1) and obtain about 999 interrupts per second. 
</li>
</ul>
<p>
<h4><a name="section_10">Platform-specific notes</a></h4>

<h5><a name="section_11">Cray XE and XK</a></h5>

When using dynamically linked binaries on Cray XE and XK systems, you 
should add the <tt>HPCTOOLKIT</tt> environment variable to your launch 
script. Set <tt>HPCTOOLKIT</tt> to the top-level <tt>HPCToolkit</tt> install 
prefix (the directory containing the <tt>bin</tt>,
<tt>lib</tt>
and 
<tt>libexec</tt>
subdirectories) and export it to the environment. This is 
only needed for running dynamically linked binaries. For example: 
<p>
<pre>
#!/bin/sh
#PBS -l mppwidth=#nodes
#PBS -l walltime=00:30:00
#PBS -V

export HPCTOOLKIT=/path/to/hpctoolkit/install/directory

    ...Rest of Script...
</pre>
<p>
If <tt>HPCTOOLKIT</tt> is not set, you may see errors such as the 
following in your job's error log. 
<p>
<pre>
/var/spool/alps/103526/hpcrun: Unable to find HPCTOOLKIT root directory.
Please set HPCTOOLKIT to the install prefix, either in this script,
or in your environment, and try again.
</pre>
<p>
The problem is that the Cray job launcher copies the <tt>hpcrun</tt>
script to a directory somewhere below <tt>/var/spool/alps/</tt>
and runs 
it from there. By moving <tt>hpcrun</tt>
to a different directory, this 
breaks <tt>hpcrun</tt>'s
method for finding its own install directory. 
The solution is to add <tt>HPCTOOLKIT</tt> to your environment so that 
<tt>hpcrun</tt>
can find its install directory. 
<p>
<h4><a name="section_12">Miscellaneous</a></h4>

<p>
<ul compact>
<li><tt>hpcrun</tt> uses preloaded shared libraries to initiate profiling. For this reason, it cannot be used to profile setuid programs.
</li>
<li><tt>hpcrun</tt> may not be able to profile programs that themselves use preloading.
</li>
</ul>
<p>
<h2><a name="section_13">See Also</a></h2>

<p>
<a href="hpctoolkit.html"><em>hpctoolkit</em>(1)
</a>.<br>

<a href="hpclink.html"><em>hpclink</em>(1)
</a>.
<p>
<h2><a name="section_14">Version</a></h2>

<p>
Version: 2017.11 of  2014/03/05.
<p>
<h2><a name="section_15">License and Copyright</a></h2>

<p>
<dl compact>
<dt>Copyright</dt>
<dd> &copy; 2002-2017, Rice University. 
</dd>
<dt>License</dt>
<dd> See <tt>README.License</tt>.
</dd>
</dl>
<p>
<h2><a name="section_16">Authors</a></h2>

<p>
John Mellor-Crummey <br>

Nathan Tallent <br>

Mark Krentel <br>

Mike Fagan <br>

Rice HPCToolkit Research Group <br>

Email: <a href ="mailto:hpctoolkit-forum =at= rice.edu"><tt>hpctoolkit-forum =at= rice.edu</tt></a>
<br>
WWW: <a href ="http://hpctoolkit.org"><tt>http://hpctoolkit.org</tt></a>.
<p>
</body>
</html>
<!-- NOTE: This file is generated, DO NOT EDIT. -->
